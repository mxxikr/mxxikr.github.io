---
title:  "Raid Level과 구축"
author:
  name: mxxikr
  link: https://github.com/mxxikr"
date: 2022-11-04 01:00:00 +0900
category:
  - [OS, Linux]
tags:
  - [os, linux, raid]
math: true
mermaid: true
---
# Raid와 Raid Level
---
### **Raid(Redundant Array of Independent Disk)**
- 2개 이상의 디스크를 병렬로 처리하여 성능 및 안정성을 향상시키는 방식
- 디스크 오류나 데이터 손실 등 장애에 대비하기 위해 복수의 디스크를 구성하는 방식
- 동일한 데이터를 여러 개의 디스크에 중복 저장하면서, 운영 체제에서는 하나의 단일 디스크로 인식

### **Raid Level**  

|레벨|방식|최대 하드 수|구성|설명|
|:--:|:--:|:--:|:--:|:--:|
|**0**|하드 연결 (stripe)|2개 이상|100M + 100M = 200M|단순 하드 연결, 하드 이중화|
|**1**|동기화 (mirroring)|2개 이상|100M + 100M = 100M|동기화 된 하드 대체 연결 (spair)|
|**2**|백업 + DATA를 각각 사용|2개 이상|100M + 100M = 100M|백업된 내용으로 복구 (사용 안함)|
|**3**|패리티 영역 추가 (byte)|3개 이상|100M + 100M + 100M = 200M|오류 기록 하드 영역 1개 추가, 한개의 하드 에러 시 복구, 패리티 영역 손상 시 복구 불가|
|**4**|패리티 영역 추가 (block)|3개 이상|100M + 100M + 100M = 200M|오류 기록 하드 영역 1개 추가, 한개의 하드 에러 시 복구, 패리티 영역 손상 시 복구 불가|
|**5**|각 하드에 분산 패리티 영역 할당|3개 이상|100M +100M + 100M(p) = 200M|패리티 영역을 각 하드디스크에 분산시킴, 랜덤한 하드 1개의 손상에 대한 복구 가능|
|**6**|각 하드에 분산 패리티 영역 할당|4개 이상|100M + 100M + 100M(p) + 100M(p) = 200M|패리티 영역을 각 하드 디스크에 분산시킴, 랜덤한 하드 2개의 손상에 대한 복구 가능|

### **RAID 0 (Striping)**
- 두개 이상의 디스크에 데이터를 순차적으로 저장하는 방식 (데이터 분산 처리)
- 복수 개의 디스크에 데이터를 분산해 처리
- 패리티 (오류 검출기능) 없이 striping 형태  
  ![image](/assets/img/linux/raid-1.jpg)  
- 장점
  - 처리속도 향상
- 단점
  - 하나의 디스크에 문제가 발생하면 raid 0으로 구성된 모든 데이터 유실

### **RAID 1 (Mirroring)**
- 데이터 처리 시, 동일한 디스크에 똑같이 저장하는 방식 (데이터 복제)
- 데이터 중복 기록하여 저장
- mirroring  
  ![image](/assets/img/linux/raid-2.jpg) 
- 장점
  - 디스크 장애 및 오류 발생시 다른 디스크를 통해 데이터 쉽게 복구 가능 (가용성, 안정성 ↑)
- 단점
  - 중복 저장하기 때문에 실질적으로 사용할 수 있는 용량 줄어듬

### **RAID 2**
- 데이터 분산 처리 방식 + 에러체크와 수정할 수 있도록 해밍코드 사용
- 하드 디스크에서 ECC(Error Correction Code)를 지원하지 않기 때문에 ECC를 별도의 드라이브에 저장함
- 기록용 드라이브와 데이터 복구용 드라이브 별도로 둠
- 에러 체크와 수정할 수 있는 Hamming Code 사용
- 최소 디스크 3개 필요  
  ![image](/assets/img/linux/raid-3.jpg) 
- 단점
  - 4개의 하드 디스크에 기록하기 위해 3개의 부가 데이터를 기록해야해 효율성 떨어짐

### **RAID 3 (Parity)**
- RAID 0 + 패리티 전용 디스크
- 데이터 분산 처리 방식 사용 + 에러 체크 및 수정을 위한 패리티 정보를 별도의 디스크에 따로 저장
- Byte 단위로 데이터 저장  
  ![image](/assets/img/linux/raid-4.jpg)   
- 전체 디스크 n개 = 실제 사용 디스크 n - 1개
- 장점
  - 오류 발생 시 패리티 정보를 통해 복구 가능
- 단점
  - 패리티 정보가 저장된 디스크 손실 시 복구 어려움

### **RAID 4 (Parity)**
- RAID 0 + 패리티 전용 디스크
- 데이터 분산 처리 방식 사용 + 에러 체크 및 수정을 위한 패리티 정보를 별도의 디스크에 따로 저장
- Block 단위로 데이터를 저장    
  ![image](/assets/img/linux/raid-5.jpg)  
- 전체 디스크 n개 = 실제 사용 디스크 n - 1개
- 장점
  - RAID 0 대비 높아진 안정성
  - RAID 1 대비 뛰어난 성능
  오류 발생 시 패리티 정보를 통해 복구 가능
- 단점
  - 패리티 정보가 저장된 디스크 손실 시 복구 어려움

### **RAID 5 (Distribute Parity)**
- RAID 3, 4의 취약점 (병목 현상) 보완하기 위해 나온 방식
- Parity bit를 분산하여 구성하여 병목 현상 줄일 수 O
- 패리티 정보를 보관하는 디스크 구성 없이 일정한 로직에 의해 데이터 분산 처리 + 에러 체크
- 서버/워크스테이션에서 가장 많이 사용
- 기본적으로 HDD 3개 이상 필요
- 1개의 랜덤한 하드 손상에 대한 복구 지원   
  ![image](/assets/img/linux/raid-6.jpg)    
    - Disk 3이 고장났을 경우 Disk 3의 D3, D8, D11를 나머지 Disk의 parity가 알고 있기 때문에 데이터 손실 X
- 전체 디스크 n개 = 실제 사용 디스크 n - 1개
- 장점
  - RAID 4 대비 병목 현상 줄임, 성능, 안정성 모두 고려해 자주 사용
- 단점
  - 1개의 디스크가 손상 된 후 바로 다른 디스크가 손상이 된다면 전체 디스크 데이터 사용 불가
  - 데이터를 읽을 때 흩어져 있는 패리티 정보를 갱신하며 읽기 때문에 성능 저하 생김

### **RAID 6**
- Parity bit를 두개의 디스크에 두어 안정성 강화  
  ![image](/assets/img/linux/raid-7.jpg)    
- 전체 디스크 n개 = 실제 사용 디스크 n - 2개
- 장점
  - RAID 4,5 대비 안정성 증가
- 단점
  - RAID 4,5 대비 비용 증가, 래피티 정보를 이중으로 저장하기 때문에 쓰기 작업 구현이 복잡

### **RAID 0 + 1**
- 최소 4개의 디스크 필요
- RAID 0(striping)과 1(mirroring) 결합 방식
- RAID 0으로 구성된 디스크들을 다시 RAID 1로 구성  
  - 4 * n개 디스크 구성시 RAID 1 + 0과 성능 차이 없음   
    ![image](/assets/img/linux/raid-8.jpg)    
  - 6개 디스크 구성시 1 개의 디스크만 고장도 나머지 디스크까지 데이터 전체 복구해야함  
    ![image](/assets/img/linux/raid-9.jpg)   
- 장점
  - 분산 저장을 통한 성능 향상, 데이터 안정성 보장
- 단점
  - 전체 용량 50%만 사용, 비용 높음

### **RAID 1 + 0**
- 최소 4개의 디스크 필요
- RAID 0(striping)과 1(mirroring) 결합 방식
- RAID 1로 구성된 디스크들을 다시 RAID 0으로 구성  
  - 4 * n개 디스크 구성시 RAID 0 + 1과 성능차이 없음   
    ![image](/assets/img/linux/raid-10.jpg)     
  - 6개 디스크 구성시 1개의 디스크가 고장나면 mirroring으로 묶힌 디스크를 통해 데이터 복구
    ![image](/assets/img/linux/raid-10.jpg)      
- 장점
  - 분산 저장을 통한 성능 향상, 데이터 안정성 보장
- 단점
  - 전체 용량 50%만 사용, 비용 높음
<br/><br/>

# Raid 관련 명령어
---
### **Raid 생성**
- **mdadm(Multi DeviceADMin)**
- `mdadm --create(-C) [raid 장치명]`
	- raid 생성할 장치
- `mdadm --level(-l)=[raid level]`
	- raid 레벨 지정
- `mdadm --raid-devices(-n)=[사용할 device 갯수] [HDD명] [HDD명]`
	- HDD 개수, 디바이스 명
- `mdadm [device명] --add [파티션]`
	- raid 장치에 파티션 추가
- `mdadm --detail(-D) [device명]`
	- raid 장치의 상세내역 출력
- `mdadm --detail --scan`
	- mdadm으로 만든 raid 장치 확인
- `mdadm [raid 장치명] -f [디스크명]`
	- 디바이스 비활성
- `mdadm [raid 장치명] -r [디스크명]`
	- 디바이스 삭제
- `mdadm [raid 장치명] -a [디스크명]`
	- 디바이스 추가,교체
- `mdadm --zero-superblock [HDD]`
	- raid 하드 삭제
- `mdadm --stop(-S) [raid 장치명]`
	- raid 장치 비활, 중지
- `mdadm --run [raid 장치명]`
	- raid 장치 작동

### **Raid 확인**
- `cat /proc/mdstat`
  - raid 구동 확인
- `mdadm --detail -scan [raid 장치명]`
  - raid 구동, 동작 확인
- `fdisk -l [raid 장치명]`
  - raid 구축 확인
- `ls -l [raid 장치명]`
  - raid 확인

### **Raid 장치 해제**
- `umount [raid 장치명]`
  - mount 해제 후 삭제
- `mdadm --stop [raid 장치명]`
  - 일정 시간 경과 후 다시 활성화 → 장치 빼거나 0으로 채움

### **Raid 고정값 설정**
1. `mdadm --detail --scan [device] > /etc/mdadm.conf`
  - 재부팅 시 raid 숫자 고정
2. `mdadm -D -s [device] > /etc/mdadm.conf`
  - 재부팅 시 raid 숫자 고정
3. `vi /etc/mdadm.conf` → 장치 추가
  - 재부팅시 raid 숫자 고정
<br/><br/>

# Raid 구축 실습
----
* **조건**
  1. 1GB 하드 디스크 5개 추가
  2. raid 1 설정하고 /raid1 디렉토리에 mount
  3. raid 5 설정하고 /raid1 디렉토리에 mount
  4. auto mount 설정  
  
1. 하드 디스크 추가
    - **edit virtual machine setting** → **add**→ **hard disk** → **scsi**→ **HDD 1G 5개 추가**→ **power on**
2. 디스크 확인
    ```bash
    fdisk -l
    ```
    
3. mdadm 패키지 설치
    
    ```bash
    yum install -y mdadm*
    ```
    
4. raid level 설정
    - HDD sd[b-c] 2개 사용해 level 1인 /dev/md1이란 raid 구축
        
        ```bash
        mdadm --create /dev/md1 --level 1 -n 2 /dev/sd[b-c]
        ```
        
        - /dev/md1를 참조하는 모든 작업이 /dev/sd[b-c]로 보내짐
    - HDD sd[d-f] 3개 사용해 level 5인 /dev/md5이란 raid 구축
        
        ```bash
        mdadm -C /dev/md5 -l 5 -n 3 /dev/sd[d-f]
        ```
        
        - /dev/md5를 참조하는 모든 작업이 /dev/sd[d-f]로 보내짐
5. raid 장치 확인
    
    ```bash
    mdadm --detail /dev/md1 # raid 장치 md1의 상세 내역 출력, 동작 확인
    mdadm -D /dev/md5 # raid 장치 md5의 상세 내역 출력, 동작 확인
    ```
    
    - **`Raid Level : raid1`,** `Raid Level : raid5`
        - 장치 raid 레벨 확인
    - `Raid Devices : 2`, `Raid Devices : 3`
        - raid device 개수 확인
    - `0 8 16 0 active sync /dev/sdb`, `1 8 32 1 active sync /dev/sdc`
        - raid device 확인
    - `0 8 48 0 active sync /dev/sdd`, `1 8 64 1 active sync /dev/sde`, `3 8 80 2 active sync /dev/sdf`
        - raid device 확인
6. raid 구축 확인
    
    ```bash
    fdisk -l /dev/md1 /dev/md5
    ```
    
7. raid 프로세스 확인
    
    ```bash
    cat /proc/mdstat
    ```
    
    - **`md5 : active raid5 sdf[3] sde[1] sdd[0]`, `[3/3] [UUU]`**
    - `md1 : active raid1 sdc[1] sdb[0]`, `[2/2] [UU]`
8. raid 장치 추가 및 확인
    
    ```bash
    mdadm -D -s /dev/md1 > /etc/mdadm.conf # raid 1 device 추가
    mdadm -D -s /dev/md5 >> /etc/mdadm.conf # raid 5 device 추가
    cat /etc/mdadm.conf # 시스템 시작시 raid array 읽는 파일
    ```
    
    - **`ARRAY /dev/md1`**
    - **`ARRAY /dev/md5`**
9. 자동 mount
    
    ```bash
    mkdir -p /raid1 /raid5 # mount point로 지정할 directory 생성
    mkfs.ext4 /dev/md1 # /dev/md1 파티션을 포맷
    mkfs.ext4 /dev/md5 # /dev/md5 파티션을 포맷
    blkid | grep md1 # 파일 시스템 타입 구성 확인 # /dev/md1: UUID="0747a647-6974-46b7-90c9-4c52b91b9d4f" TYPE="ext4" 
    blkid | grep md5 # 파일 시스템 타입 구성 확인 # /dev/md5: UUID="f13c3b26-b812-44e9-b8dd-df02e9e1f7fd" TYPE="ext4"
    vi /etc/fstab # mount 설정을 영구적으로 할 수 있도록 존재하는 설정 파일
    ```
    
    ```bash
    UUID=0747a647-6974-46b7-90c9-4c52b91b9d4f /raid1 ext4 defaults 1 2
    
    UUID=f13c3b26-b812-44e9-b8dd-df02e9e1f7fd /raid5 ext4 defaults 1 2
    
    :.!blkid |grep md1 && blkid |grep md5    # md1, md5 포함 문자열 추출
    ```
    
    ```bash
    mount -a # /etc/fstab에 있는 모든 내용 mount
    df -h # mount 정보 출력
    init 6 # 재부팅 후 확인
    ```
    
10. raid 프로세스 확인
    
    ```bash
    cat /proc/mdstat # raid 프로세스 확인
    mdadm -D /dev/md1 /dev/md5 # raid 장치 상세 내역 출력, 동작 확인
    ```